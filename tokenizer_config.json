{"class_name": "Tokenizer", "config": {"num_words": 5000, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": "<OOV>", "document_count": 2, "word_counts": "{\"sample\": 1, \"message\": 1, \"another\": 1, \"spam\": 1, \"text\": 1}", "word_docs": "{\"sample\": 1, \"message\": 1, \"another\": 1, \"text\": 1, \"spam\": 1}", "index_docs": "{\"2\": 1, \"3\": 1, \"4\": 1, \"6\": 1, \"5\": 1}", "index_word": "{\"1\": \"<OOV>\", \"2\": \"sample\", \"3\": \"message\", \"4\": \"another\", \"5\": \"spam\", \"6\": \"text\"}", "word_index": "{\"<OOV>\": 1, \"sample\": 2, \"message\": 3, \"another\": 4, \"spam\": 5, \"text\": 6}"}}